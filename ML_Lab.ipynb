{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE3oXMLf89VrHkOQpE8it3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanushreeNayak/ML-programs/blob/main/ML_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find-S algorithm"
      ],
      "metadata": {
        "id": "egp9stzhWaBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVn7bT5dKHkU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "csv_file_data = pd.read_csv('/DataS.csv')\n",
        "\n",
        "data = np.array(csv_file_data)\n",
        "\n",
        "def trainModel(trainingData):\n",
        "  n = len(trainingData[0]) - 1\n",
        "  specific_hypo = [\"&\"] * n\n",
        "  general_hypo = [\"?\"] * n\n",
        "\n",
        "  for instance in trainingData:\n",
        "    if instance[-1] == 'yes':\n",
        "      specific_hypo = instance[:-1]\n",
        "      print('specific_hypo : ',specific_hypo)\n",
        "      break\n",
        "\n",
        "  for instance in trainingData:\n",
        "    if instance[-1] == 'no':\n",
        "      for i in range(n):\n",
        "        if specific_hypo[i] != instance[i]:\n",
        "          specific_hypo[i] = '?'\n",
        "      print('Hypothesis : ',specific_hypo)\n",
        "\n",
        "      if(specific_hypo == general_hypo):\n",
        "        break\n",
        "\n",
        "trainModel(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate Elimination Algorithm"
      ],
      "metadata": {
        "id": "_z-ffnNbV5CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "csv_file_data = pd.read_csv(\"/DataS.csv\")\n",
        "# print(csv_file_data)\n",
        "\n",
        "data = np.array(csv_file_data)\n",
        "# print(data)\n",
        "\n",
        "# Separate Features and Labels\n",
        "X = data[:,:-1] # Features\n",
        "Y = data[:,-1] # Labels\n",
        "# print(x)\n",
        "# print(y)\n",
        "\n",
        "# Initialize Most General and Most Specific Hypothesis\n",
        "num_features = X.shape[1]\n",
        "# print(num_fearures)\n",
        "S = ['Ø']*num_features # Most specific hypothesis\n",
        "G = [['?']*num_features] # Most general hypothesis\n",
        "# print(S)\n",
        "# print(G)\n",
        "\n",
        "# Candidate Elimination Algorithm\n",
        "for i, x in enumerate(X):\n",
        "    if Y[i] == 'Yes':  # Positive Example\n",
        "        for j in range(num_features):\n",
        "            if S[j] == 'Ø':  # Initialize S\n",
        "                S[j] = x[j]\n",
        "            elif S[j] != x[j]:  # Generalize S\n",
        "                S[j] = '?'\n",
        "        G = [g for g in G if all(g[j] == '?' or g[j] == x[j] for j in range(num_features))]\n",
        "\n",
        "    else:  # Negative Example\n",
        "        new_G = []\n",
        "        for g in G:\n",
        "            for j in range(num_features):\n",
        "                if g[j] == '?':\n",
        "                    g_new = g[:]\n",
        "                    g_new[j] = S[j]  # Specialize G\n",
        "                    new_G.append(g_new)\n",
        "        G = new_G\n",
        "        G = [g for g in G if not all(g[j] == '?' or g[j] == x[j] for j in range(num_features))]\n",
        "\n",
        "# Output Final S and G\n",
        "print(\"Final Specific Hypothesis (S):\", S)\n",
        "print(\"Final General Hypothesis (G):\", G)"
      ],
      "metadata": {
        "id": "IX4mr2iiV3kd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate Elimination Algorithm version 2"
      ],
      "metadata": {
        "id": "4ZD0mz1hXVNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "file = open('/DataS.csv')\n",
        "all_rows = csv.reader(file)\n",
        "\n",
        "attributes = [[\"Big\", \"Small\"], [\"Blue\", \"Red\"], [\"Circle\", \"Triangle\"]]\n",
        "# attributes = [[\"Sunny\", \"Rainy\"], [\"Warm\", \"Cold\"], [\"Normal\", \"High\"], [\"Strong\", \"Weak\"], [\"Warm\", \"Cold\"], [\"Same\", \"Change\"]]\n",
        "\n",
        "data = []\n",
        "for row in all_rows:\n",
        "    data.append(row)\n",
        "\n",
        "specific_hypothesis = []\n",
        "genral_hypothesis = []\n",
        "\n",
        "for instance_index, instance in enumerate(data):\n",
        "    print(genral_hypothesis, \"gen\")\n",
        "    print(specific_hypothesis, \"spe\")\n",
        "\n",
        "    if instance[-1]==\"Yes\":\n",
        "        pop_index=[]\n",
        "        new_genral_hypothesis=[]\n",
        "        for hypothesis_index, hypothesis in enumerate(genral_hypothesis):\n",
        "            for attribute_index, attribute in enumerate(hypothesis):\n",
        "\n",
        "                if genral_hypothesis[hypothesis_index][attribute_index] != \"?\" and genral_hypothesis[hypothesis_index][attribute_index] != instance[attribute_index] and hypothesis_index not in pop_index:\n",
        "                    pop_index.append(hypothesis_index)\n",
        "                    # print(genral_hypothesis.pop(hypothesis_index), \"pop\")\n",
        "\n",
        "        for hypo_indx in range(len(genral_hypothesis)):\n",
        "            if hypo_indx not in pop_index:\n",
        "                new_genral_hypothesis.append(genral_hypothesis[hypo_indx])\n",
        "\n",
        "        genral_hypothesis = new_genral_hypothesis\n",
        "\n",
        "        if specific_hypothesis:\n",
        "            for attribute_index, attribute in enumerate(specific_hypothesis):\n",
        "                if specific_hypothesis[attribute_index] != instance[attribute_index]:\n",
        "                     specific_hypothesis[attribute_index] = '?'\n",
        "        else:\n",
        "            specific_hypothesis = instance[:-1]\n",
        "\n",
        "    elif instance[-1]==\"No\":\n",
        "        if genral_hypothesis:\n",
        "            opp_instance = []\n",
        "            for attribute_index, attribute in enumerate(instance):\n",
        "                if attribute_index<len(data[0])-1:\n",
        "\n",
        "                    if instance[attribute_index] == attributes[attribute_index][0]:\n",
        "                        opp_instance.append(attributes[attribute_index][1])\n",
        "                    else:\n",
        "                        opp_instance.append(attributes[attribute_index][0])\n",
        "\n",
        "            new_genhypo=[]\n",
        "            for hypothesis_index, hypothesis in enumerate(genral_hypothesis):\n",
        "                for attribute_index, attribute in enumerate(hypothesis):\n",
        "\n",
        "                    if genral_hypothesis[hypothesis_index][attribute_index] != \"?\" and genral_hypothesis[hypothesis_index][attribute_index] != opp_instance[attribute_index]:\n",
        "                        temp = genral_hypothesis.pop(hypothesis_index)\n",
        "                        # pop_index.append(hypothesis_index)\n",
        "\n",
        "                        for element_index, element in enumerate(opp_instance):\n",
        "                            temp_hypo=[\"?\"]*len(temp)\n",
        "                            if element_index == attribute_index:\n",
        "                                pass\n",
        "                            else:\n",
        "                                temp_hypo[attribute_index] = temp[attribute_index]\n",
        "                                temp_hypo[element_index] = element\n",
        "                                new_genhypo.append(temp_hypo)\n",
        "\n",
        "            if new_genhypo:\n",
        "                for hypo in new_genhypo:\n",
        "                    if hypo not in genral_hypothesis:\n",
        "                        genral_hypothesis.append(hypo)\n",
        "\n",
        "        else:\n",
        "            for attribute_index in range(len(data[0])-1):\n",
        "\n",
        "                temp = [\"?\"]*(len(data[0])-1)\n",
        "\n",
        "                if attribute_index < len(attributes):\n",
        "                    if instance[attribute_index] == attributes[attribute_index][0]:\n",
        "                        temp[attribute_index] = attributes[attribute_index][1]\n",
        "                    else:\n",
        "                        temp[attribute_index] = attributes[attribute_index][0]\n",
        "\n",
        "                genral_hypothesis.append(temp)\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "print(specific_hypothesis)\n",
        "print(genral_hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj39Y-w5XU6I",
        "outputId": "50851a2e-dabb-42dc-9e1b-72a8428a8d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] gen\n",
            "[] spe\n",
            "[] gen\n",
            "[] spe\n",
            "[] gen\n",
            "['Morning', 'Sunny', 'Warm', 'Yes', 'Mild', 'Strong'] spe\n",
            "[['Big', '?', '?', '?', '?', '?'], ['?', 'Blue', '?', '?', '?', '?'], ['?', '?', 'Circle', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']] gen\n",
            "['Morning', 'Sunny', 'Warm', 'Yes', 'Mild', 'Strong'] spe\n",
            "[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']] gen\n",
            "['Morning', 'Sunny', '?', 'Yes', '?', '?'] spe\n",
            "['?', 'Sunny', '?', 'Yes', '?', '?']\n",
            "[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List Then Elimination Algorithm"
      ],
      "metadata": {
        "id": "fFWZXSSus8Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "\n",
        "csv_file_data=pd.read_csv('/DataS.csv')\n",
        "#print(csv_file_data)\n",
        "\n",
        "def create_sementical_distinct_hypothesis(data):\n",
        "    # Extract the examples of the DataSet\n",
        "    examples=np.array(data)[:,:-1]\n",
        "    # print(examples)\n",
        "\n",
        "    number_of_features=len(examples[0])\n",
        "    #print(number_of_features)\n",
        "\n",
        "    # Transpose the examples so that it is easy to make the set of the values of feature\n",
        "    feature_transpose=np.transpose(examples)\n",
        "    # print(feature_transpose)\n",
        "\n",
        "    # Declare a 2D Array which store all the features set\n",
        "    feature=[[]]*number_of_features\n",
        "    # print(feature)\n",
        "\n",
        "    # Iterations which make the sets of features value and also convert them into the 2D Array\n",
        "    for i in range(len(feature_transpose)):\n",
        "        my_set=set()\n",
        "        for j in range(len(feature_transpose[0])):\n",
        "            my_set.add(feature_transpose[i][j])\n",
        "        # print(my_set)\n",
        "        feature[i]=list(my_set)\n",
        "    # print(feature)\n",
        "\n",
        "    # Append the general condition in all the feature vectors\n",
        "    for i in range(len(feature)):\n",
        "        feature[i].append('?')\n",
        "    # print(feature)\n",
        "\n",
        "    # Create the Sementical Distinct Hypothesis\n",
        "    sementical_distinct_hypothesis=list(product(*feature))\n",
        "    # print(sementical_distinct_hypothesis)\n",
        "    # print(len(sementical_distinct_hypothesis))\n",
        "\n",
        "    # Return the Sementical Distinct Hypothesis\n",
        "    return sementical_distinct_hypothesis\n",
        "\n",
        "def compare_feature_Vector(arr1,arr2):\n",
        "    # Check the corresponding feature value is acceptable or not\n",
        "    # if acceptable return True otherwise return False\n",
        "    length=len(arr1)\n",
        "    for i in range(length):\n",
        "        if arr1[i]==arr2[i]:\n",
        "            pass\n",
        "        elif arr1[i]=='?':\n",
        "            pass\n",
        "        else:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def check_hypothesis(hypothesis,instance,target):\n",
        "    # Check Hypothesis is consistent or not\n",
        "    # If consistent return True otherwise return False\n",
        "    for i,val in enumerate(instance):\n",
        "        if compare_feature_Vector(hypothesis,val):\n",
        "            if target[i]=='Yes':\n",
        "                pass\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            if target[i]=='No':\n",
        "                pass\n",
        "            else:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def create_version_space(csv_file_data):\n",
        "    semantical_distinct=create_sementical_distinct_hypothesis(csv_file_data)\n",
        "\n",
        "    data=np.array(csv_file_data)[:,:-1]\n",
        "    # print(data)\n",
        "\n",
        "    target=np.array(csv_file_data)[:,-1]\n",
        "    # print(target)\n",
        "\n",
        "    # It appends all the consistent hypothesis to the Version Space\n",
        "    version_space=[]\n",
        "    for i in semantical_distinct:\n",
        "        if check_hypothesis(i,data,target):\n",
        "            version_space.append(i)\n",
        "    return version_space\n",
        "\n",
        "version_space=create_version_space(csv_file_data)\n",
        "print(version_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGa_24HotLbV",
        "outputId": "509b9b4e-edab-4234-baeb-fbb10780d5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('?', 'Sunny', '?', 'Yes', '?', '?'), ('?', 'Sunny', '?', '?', '?', '?'), ('?', '?', '?', 'Yes', '?', '?')]\n"
          ]
        }
      ]
    }
  ]
}